{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justice in a Big Data Society: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Why COMPAS?\n",
    "\n",
    "\n",
    "Propublica started the COMPAS Debate with the article [Machine Bias](#References).  With their article, they also released details of their methodology and their [data and code](https://github.com/propublica/compas-analysis).  This presents a real data set that can be used for research on how data is used in a criminal justice settingn without researchers having to perform their own requests for information, so it has been used and reused a lot of times. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A COMPAS Case study\n",
    "\n",
    "Next, let's look at what COMPAS is before we look at the data. \n",
    "\n",
    "The COMPAS score comes from the results of a [137 item survey](compas-core-sample.pdf).  It is distributed with a long [Practitioner's guide](compas_guide.pdf) that describes how it was developed and validated including which criminal theories it relies on.  \n",
    "\n",
    "The claim is that COMPAS predicts two-year recidivism.  It has an accuracy around 67%. How does knowing that it comes from that survey impact your view of it?  What problems might you predict? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propublica COMPAS Data\n",
    "\n",
    "The dataset consists of COMPAS scores assigned to defendants over two years 2013-2014 in Broward County, Florida. These scores are determined by a proprietary algorithm designed to evaluate a persons recidivism risk - the likelihood that they will reoffend. Risk scoring algorithms are widely used by judges to inform their scentencing and bail decisions in the criminal justice system in the United States. The original ProPublica analysis identified a number of fairness concerns around the use of COMPAS scores, including that ''black defendants were nearly twice as likely to be misclassified as higher risk compared to their white counterparts.'' Please see the full article for further details.\n",
    "\n",
    "\n",
    "\n",
    "Let's get started by importing the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import roc_curve\n",
    "from utilities import *\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "\n",
    "First we import the COMPAS dataset from the propublica repo and store it in a Pandas [dataframe](https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/propublica/compas-analysis/raw/master/compas-scores-two-years.csv\", \n",
    "                 header=0).set_index('id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at this dataset. We print the features, and then the first entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "For this analysis, we will restrict ourselves to only a few features, and clean the dataset according to the methods using in the original ProPublica analysis. \n",
    "\n",
    "Details of the cleaning method can be found in the utilities file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features that will be analyzed\n",
    "features_to_keep = ['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', \n",
    "                    'days_b_screening_arrest', 'decile_score', 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']\n",
    "df = df[features_to_keep]\n",
    "df = clean_compas(df)\n",
    "df.head()\n",
    "print(\"\\ndataset shape (rows, columns)\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Next we provide a few ways to look at the relationships between the attributes in the dataset. Here is an explanation of these values:\n",
    "\n",
    "* `age`: defendant's age\n",
    "* `c_charge_degree`: degree charged (Misdemeanor of Felony)\n",
    "* `race`: defendant's race\n",
    "* `age_cat`: defendant's age quantized in \"less than 25\", \"25-45\", or \"over 45\"\n",
    "* `score_text`: COMPAS score: 'low'(1 to 5), 'medium' (5 to 7), and 'high' (8 to 10).\n",
    "* `sex`: defendant's gender\n",
    "* `priors_count`: number of prior charges\n",
    "* `days_b_screening_arrest`: number of days between charge date and arrest where defendant was screened for compas score\n",
    "* `decile_score`: COMPAS score from 1 to 10 (low risk to high risk)\n",
    "* `is_recid`: if the defendant recidivized\n",
    "* `two_year_recid`: if the defendant within two years\n",
    "* `c_jail_in`: date defendant was imprisoned\n",
    "* `c_jail_out`: date defendant was released from jail\n",
    "* `length_of_stay`: length of jail stay\n",
    "\n",
    "In particular, as in the ProPublica analysis, we are interested in the implications for the treatment of different groups as defined by some **sensitive data attributes**. In particular we will consider race as the protected attribute in our analysis. Next we look at the number of entries for each race.\n",
    "\n",
    "\n",
    "<font color=red> Another interesting fairness analysis might be to consider group outcomes by gender or age. In fact, a [recent appeal to the supreme court](https://en.wikipedia.org/wiki/Loomis_v._Wisconsin) challenged the role of gender in determining COMPAS scores.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict our analysis to African-American and Caucasian, since we have significantly more samples for these two groups. We remove entries not marked as African-American or Caucasian, and look at the distribution of COMPAS scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['race'].isin(['African-American','Caucasian'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPAS score distribution\n",
    "\n",
    "Let's look at the COMPAS score distribution between African-Americans and Caucasians (matches the one in the ProPublica article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_score_table = df.groupby(['race','decile_score']).size().reset_index().pivot(index='decile_score',columns='race',values=0)\n",
    "\n",
    "# percentage of defendants in each score category\n",
    "(100*race_score_table/race_score_table.sum()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in visual form\n",
    "x = df.loc[df['race']=='African-American','decile_score'].values\n",
    "y = df.loc[df['race']=='Caucasian','decile_score'].values\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.hist([x,y],normed=True,bins = np.arange(-.5,10.5,1))\n",
    "plt.legend(['African-American','Caucasian'])\n",
    "plt.title('COMPAS score distribution')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Fraction of sample');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, there is a large discrepancy. Does this change when we condition on other random variables? We'll start with priors and look at how those are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df.loc[df['race']=='African-American','priors_count'].values\n",
    "y = df.loc[df['race']=='Caucasian','priors_count'].values\n",
    "plt.figure(figsize=[12,7])\n",
    "\n",
    "plt.hist([x,y],normed=True,bins = max(np.unique(np.concatenate((x,y)))))\n",
    "plt.legend(['African-American','Caucasian'])\n",
    "plt.title('Priors distribution by Race')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Fraction of sample');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of scores for individuals with more than 2 priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2priors = df.loc[df['priors_count']>=2]\n",
    "x = df_2priors.loc[df_2priors['race']=='African-American','decile_score'].values\n",
    "y = df_2priors.loc[df_2priors['race']=='Caucasian','decile_score'].values\n",
    "plt.figure(figsize=[12,7])\n",
    "plt.hist([x,y],normed=True)\n",
    "plt.legend(['African-American','Caucasian'])\n",
    "plt.title('COMPAS score distribution for defendants with more than 2 priors')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Fraction of sample');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a big difference. What about those with less than two priors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2priors = df.loc[df['priors_count']<2]\n",
    "x = df_2priors.loc[df_2priors['race']=='African-American','decile_score'].values\n",
    "y = df_2priors.loc[df_2priors['race']=='Caucasian','decile_score'].values\n",
    "plt.figure(figsize=[12,7])\n",
    "plt.hist([x,y],normed=True)\n",
    "plt.legend(['African-American','Caucasian'])\n",
    "plt.title('COMPAS score distribution for defendants with less than 2 priors')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Fraction of sample');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question: ###\n",
    "How does the COMPAS score distribution change under different conditionings (e.g. degree charged, length of prison stay, etc.)? Does a [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)-like phenomenom happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard to find... For example at young felons with no priors\n",
    "df_ans = df.loc[(df['priors_count']==0)&(df['c_charge_degree']=='F')&df['age']<=25]\n",
    "x = df_ans.loc[df_ans['race']=='African-American','decile_score'].values\n",
    "y = df_ans.loc[df_ans['race']=='Caucasian','decile_score'].values\n",
    "plt.figure(figsize=[12,7])\n",
    "plt.hist([x,y],normed=True)\n",
    "plt.legend(['African-American','Caucasian'])\n",
    "plt.title('COMPAS score distribution - Less than 25, charged with felony and no priors')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Fraction of population');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We quantize our dataset next to make the analysis a little bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQ = df.copy()\n",
    "\n",
    "# Quantize priors count between 0, 1-3, and >3\n",
    "def quantizePrior(x):\n",
    "    if x <=0:\n",
    "        return '0'\n",
    "    elif 1<=x<=3:\n",
    "        return '1 to 3'\n",
    "    else:\n",
    "        return 'More than 3'\n",
    "\n",
    "    \n",
    "# Quantize length of stay\n",
    "def quantizeLOS(x):\n",
    "    if x<= 7:\n",
    "        return '<week'\n",
    "    if 8<x<=93:\n",
    "        return '<3months'\n",
    "    else:\n",
    "        return '>3 months'\n",
    "    \n",
    "# Quantize length of stay\n",
    "def adjustAge(x):\n",
    "    if x == '25 - 45':\n",
    "        return '25 to 45'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# Quantize score_text to MediumHigh\n",
    "def quantizeScore(x):\n",
    "    if (x == 'High')| (x == 'Medium'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "dfQ['priors_count'] = dfQ['priors_count'].apply(quantizePrior)\n",
    "dfQ['length_of_stay'] = dfQ['length_of_stay'].apply(quantizeLOS)\n",
    "dfQ['score_text'] = dfQ['score_text'].apply(quantizeScore)\n",
    "dfQ['age_cat'] = dfQ['age_cat'].apply(adjustAge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the percentage difference of the average COMPAS scores between Caucasians and African-Americans for different groups. (**Important**: error bars omitted, so large differences may be due to limited samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# African-american\n",
    "dfPlot_AA = dfQ.loc[dfQ['race']=='African-American'].copy()\n",
    "dfPlot_AA = dfPlot_AA.groupby(['age_cat','c_charge_degree','priors_count'])['decile_score'].mean().reset_index()\n",
    "dfinal_AA = dfPlot_AA.pivot_table(index =['age_cat'],columns = ['c_charge_degree','priors_count'],values=\"decile_score\")\n",
    "\n",
    "# Caucasian\n",
    "dfPlot_C = dfQ.loc[dfQ['race']=='Caucasian'].copy()\n",
    "dfPlot_C = dfPlot_C.groupby(['age_cat','c_charge_degree','priors_count'])['decile_score'].mean().reset_index()\n",
    "dfinal_C = dfPlot_C.pivot_table(index =['age_cat'],columns = ['c_charge_degree','priors_count'],values=\"decile_score\")\n",
    "\n",
    "plt.figure(figsize=[12,7])\n",
    "# sns.heatmap(100*(dfinal_AA.div(dfinal_C)-1),cmap=\"coolwarm\",annot=True)\n",
    "f = 100*(dfinal_AA.div(dfinal_C)-1)\n",
    "plt.pcolor(f)\n",
    "plt.colorbar()\n",
    "plt.yticks([0.5,1.5,2.5],['<25', '>45', '25-45'])\n",
    "# plt.yticklabels(\n",
    "plt.ylabel(\"Age\")\n",
    "plt.xlabel(\"Number of prior charges\")\n",
    "plt.title('Average COMPAS score percentage difference between\\nCaucasian and African-American for different groups')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening with the \"greater than 45\" group? Let's plot the score distribution for that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g45 = df.loc[(df['age']>=45)]\n",
    "x = df_g45.loc[df_g45['race']=='African-American','decile_score'].values\n",
    "y = df_g45.loc[df_g45['race']=='Caucasian','decile_score'].values\n",
    "plt.figure(figsize=[12,7])\n",
    "plt.hist([x,y],normed=True)\n",
    "plt.legend(['African-American','Caucasian'])\n",
    "plt.title('COMPAS score distribution - Greater than 45 years old')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Fraction of population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when we take actual 2-year recidivism values into account? Are the predictions fair?\n",
    "\n",
    "First, let's look at the correlation between the quantized score, the decile score and the actual recidivism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between COMPAS score and 2-year recidivism\n",
    "\n",
    "# measure with high-low score\n",
    "print(dfQ[['two_year_recid','score_text']].corr())\n",
    "\n",
    "# measure with decile_score\n",
    "print(dfQ[['two_year_recid','decile_score']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is not that high. How can we evaluate whether the predictions made by the COMPAS scores are fair, especially considering that they do not predict recidivism rates well?\n",
    "\n",
    "##  Fairness Metrics\n",
    "\n",
    "The question of how to determine if an algorithm is *fair* has seen much debate recently (see this tutorial from the Conference on Fairness, Acountability, and Transparency titled [21 Fairness Definitions and Their Politics](https://fatconference.org/2018/livestream_vh220.html).\n",
    "\n",
    "And in fact some of the definitions are contradictory, and have been shown to be mutually exclusive [2,3] https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say\n",
    "\n",
    "Here we will cover 3 notions of fairness and present ways to measure them:\n",
    "\n",
    "1. **Disparate Impact** [4](#References) \n",
    "[The 80% rule](https://en.wikipedia.org/wiki/Disparate_impact#The_80.25_rule)\n",
    "\n",
    "2. **Calibration** [6](#References)\n",
    "\n",
    "4. **Equalized Odds** [5](#References) \n",
    "\n",
    "For the rest of our analysis we will use a binary outcome - COMPAS score <= 4 is LOW RISK, >4 is HIGH RISK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparate Impact\n",
    "\n",
    "Disparate impact is a legal concept used to describe situations when an entity such as an employer *inadvertently* discriminates gainst a certain protected group. This is distinct from *disparate treatment* where discrimination is intentional. \n",
    "\n",
    "To demonstrate cases of disparate impact, the Equal Opportunity Commission (EEOC) proposed \"rule of thumb\" is known as the [The 80% rule](https://en.wikipedia.org/wiki/Disparate_impact#The_80.25_rule). \n",
    "\n",
    "Feldman et al. [4](#References) adapted a fairness metric from this  principle. For our application, it states that the percent of defendants predicted to be high risk in each protected group (in this case whites and african-americans) should be within 80% of each other. \n",
    "\n",
    "Let's evaluate this standard for the COMPAS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's measure the disparate impact according to the EEOC rule\n",
    "means_score = dfQ.groupby(['score_text','race']).size().unstack().reset_index()\n",
    "means_score = means_score/means_score.sum()\n",
    "means_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute disparte impact\n",
    "AA_with_high_score = means_score.loc[1,'African-American']\n",
    "C_with_high_score = means_score.loc[1,'Caucasian']\n",
    "\n",
    "C_with_high_score/AA_with_high_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ratio is below .8, so there is disparate impact by this rule.  (Taking the priveleged group and the undesirable outcome instead of the disadvantaged group and the favorable outcome).\n",
    "\n",
    "What if we apply the same rule to the **true** two year recidivism instead of the quantized COMPAS score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_2yr = dfQ.groupby(['two_year_recid','race']).size().unstack()\n",
    "means_2yr = means_2yr/means_2yr.sum()\n",
    "means_2yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute disparte impact\n",
    "AA_with_high_score = means_2yr.loc[1,'African-American']\n",
    "C_with_high_score = means_2yr.loc[1,'Caucasian']\n",
    "C_with_high_score/AA_with_high_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference in re-arrest, but not as high as assigned by the COMPAS scores. This is still a disparate impact of the actual arrests (since this not necesarrily accurate as a recidivism rate, but it is true rearrest).\n",
    "\n",
    "Now let's measure the difference in scores when we consider both the COMPAS output and true recidivism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration \n",
    "\n",
    "A discussion of using calibration to verify the fairness of a model can be found in Northpoint's response to the ProPublica article [6](#References). \n",
    "\n",
    "The basic idea behind calibrating a classifier is that you want the confidence of the predictor to reflect the true outcomes. So, in a well-calibrated classifier, if 100 people are assigned 90% confidence of being in the positive class, then in reality, 90 of them should actually have had a positive label. \n",
    "\n",
    "To use calibration as a fairness metric we compare the calibration of the classifier for each group.  The smaller the difference, the more fair the calssifier.\n",
    "\n",
    "In our problem this can be expressed as given $Y$ indicating two year recidivism, $S_Q$ indicating score (0=low, 1=high medium), and $R$ indicating race, we measure\n",
    "\n",
    "$$\\mathsf{cal} \\triangleq \\frac{\\mathbb{P}\\left(Y=1\\mid S_Q=s,R=\\mbox{African-American} \\right)}{\\mathbb{P}\\left(Y=1 \\mid S_Q=s,R=\\mbox{Caucasian} \\right)},$$ for different scores $s$. Considering our quantized scores, we look at the calibration for $s=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages\n",
    "dfAverage = dfQ.groupby(['race','score_text'])['two_year_recid'].mean().unstack()\n",
    "dfAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = dfAverage.loc['African-American',1]\n",
    "denom = dfAverage.loc['Caucasian',1]\n",
    "cal = num/denom\n",
    "calpercent = 100*(cal-1)\n",
    "print('Calibration: %f' % cal)\n",
    "print('Calibration in percentage: %f%%' % calpercent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference looks much smaller than before. The problem of the above calibration measure is that it depends on the threshold on which we quantized the scores $S_Q$. \n",
    "\n",
    "In order to mitigate this, ine might use a variation of this measure called *predictive parity.* In this example, we define predictive parity as\n",
    "\n",
    "$$\\mathsf{PP}(s) \\triangleq \\frac{\\mathbb{P}\\left(Y=1\\mid S\\geq s,R=\\mbox{African-American} \\right)}{\\mathbb{P}\\left(Y=1 \\mid S\\geq s,R=\\mbox{Caucasian} \\right)},$$\n",
    "where $S$ is the original score.\n",
    "\n",
    "We plot $\\mathsf{PP}(s) $ for $s$ from 1 to 10. Note how predictive parity depends significantly on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux function for thresh score\n",
    "def threshScore(x,s):\n",
    "    if x>=s:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "ppv_values = []\n",
    "dfP = dfQ[['race','two_year_recid']].copy()\n",
    "for s in range(1,11):\n",
    "    dfP['threshScore'] = dfQ['decile_score'].apply(lambda x: threshScore(x,s))\n",
    "    dfAverage = dfP.groupby(['race','threshScore'])['two_year_recid'].mean().unstack()\n",
    "    num = dfAverage.loc['African-American',1]\n",
    "    denom = dfAverage.loc['Caucasian',1]\n",
    "    ppv_values.append(100*(num/denom-1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(range(1,11),ppv_values)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel('Score Threshold')\n",
    "plt.ylabel('Predictive Parity (percentage)')\n",
    "plt.title('Predictive parity for different thresholds\\n(warning: no error bars)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds\n",
    "\n",
    "The last fairness metric we consider is based on the difference in *error rates* between groups. Hardt et al. [5](#References) propose to look at the difference in the true positive and false positive rates for each group. This aligns with the analysis performed by Propublica. WE can examine these values looking at is the ROC for each group. We normalize the score between 0 and 1. The ROC thresholds produced by `scikitlearn` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize decile score\n",
    "max_score = dfQ['decile_score'].max()\n",
    "min_score = dfQ['decile_score'].min()\n",
    "dfQ['norm_score'] = (dfQ['decile_score']-min_score)/(max_score-min_score)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "#plot ROC curve for African-Americans\n",
    "y = dfQ.loc[dfQ['race']=='African-American',['two_year_recid','norm_score']].values\n",
    "fpr1,tpr1,thresh1 = roc_curve(y_true = y[:,0],y_score=y[:,1])\n",
    "plt.plot(fpr1,tpr1)\n",
    "\n",
    "#plot ROC curve for Caucasian\n",
    "y = dfQ.loc[dfQ['race']=='Caucasian',['two_year_recid','norm_score']].values\n",
    "fpr2,tpr2,thresh2 = roc_curve(y_true = y[:,0],y_score=y[:,1])\n",
    "plt.plot(fpr2,tpr2)\n",
    "l = np.linspace(0,1,10)\n",
    "plt.plot(l,l,'k--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Postitive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(['African-American','Caucasian'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each group, the point in the ROC curve corresponds to a $$(\\mbox{false postive rate, true positive rate})$$ pair for a given threshold. In order to caputre the difference in error rates, we map the points to $$\\left(\\frac{\\mbox{false postive rate Afr.-American}}{\\mbox{false postive rate Cauc.}},s \\right)$$\n",
    "and similarly for *false negative* rates for different thersholds s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_ratio = fpr1[1:]/fpr2[1:]\n",
    "tpr_ratio = (tpr1[1:])/(tpr2[1:])\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(thresh1[1:],fpr_ratio)\n",
    "plt.plot(thresh1[1:],tpr_ratio)\n",
    "plt.xlabel('Normalized score threshold')\n",
    "plt.ylabel('Ratio')\n",
    "\n",
    "plt.legend(['False positive rate','True positive rate'])\n",
    "plt.title('Ratio between African-American and Caucasian error rates for different score thresholds');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is once again stark. This graph is particlarly concerning due to the significantly higher false positive rates for African Americans across all thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Criticism\n",
    "\n",
    "The Dressel paper and others, instead criticize the system on more fundamental positions. \n",
    "\n",
    "We saw during the transparency week, the [CORELS system](https://corels.eecs.harvard.edu/corels/run.html). It learns a rule list from the Propulica data and reports similar accuracy. \n",
    "\n",
    "```\n",
    "if ({Prior-Crimes>3}) then ({label=1})\n",
    "else if ({Age=18-22}) then ({label=1})\n",
    "else ({label=0})\n",
    "```\n",
    "\n",
    "Let's investigate how that score compares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corels_rule(row):\n",
    "    return int(row['priors_count'] > 3 or (row['priors_count'] <=3 and row['age'] <=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corels'] = df.apply(corels_rule,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's measure the disparate impact according to the EEOC rule\n",
    "means_corel = df.groupby(['corels','race']).size().unstack().reset_index()\n",
    "means_corel = means_corel/means_corel.sum()\n",
    "means_corel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute disparte impact\n",
    "AA_with_high_score = means_corel.loc[1,'African-American']\n",
    "C_with_high_score = means_corel.loc[1,'Caucasian']\n",
    "\n",
    "C_with_high_score/AA_with_high_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions can you draw about this simple algorithm?  Is its behavior expected or surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Predictor\n",
    "\n",
    "_if time or for later_\n",
    "\n",
    "One part of the Dressel result was that a linear predictor in the available features as released by Propublica can also acheive similar accuracy.  Try to repeat that analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]ProPublica, *“Machine Bias,”* https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing, May 2016.\n",
    "\n",
    "[] ProPublica  https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n",
    "\n",
    "[2] A. Chouldechova. *\"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.\"* arXiv preprint arXiv:1703.00056 (2017).\n",
    "\n",
    "[3](#3) Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan. \"Inherent trade-offs in the fair determination of risk scores.\" arXiv preprint arXiv:1609.05807 (2016).\n",
    "\n",
    "[4] Feldman, Michael, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. *\"Certifying and removing disparate impact.\"* In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 259-268. ACM, 2015.\n",
    "\n",
    "[5] Hardt, Moritz, Eric Price, and Nati Srebro. *\"Equality of opportunity in supervised learning.\"* Advances in neural information processing systems. 2016.\n",
    "\n",
    "[6] Dieterich, William, Christina Mendoza, and Tim Brennan. \"COMPAS risk scales: Demonstrating accuracy equity and predictive parity.\" Northpoint Inc (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
